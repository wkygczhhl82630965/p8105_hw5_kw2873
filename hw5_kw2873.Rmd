---
title: "hw5"
author: "Keyi Wang"
date: "11/6/2019"
output: github_document
---

```{r setup, include=FALSE}
# knitr will run the chunk but not include the chunk in the final document
# copy from Jeff

# ensure reproductivity
set.seed(10)

library(tidyverse)
library(viridis)
library(ggridges)
library(patchwork)
library(rvest)

knitr::opts_chunk$set(
  # display the code in the code truck above its results in the final document
  echo = TRUE,
  # do not display any warning messages generated by the code
  warning = FALSE,
  # set the figure to be 8 x 6, and the proportion it takes to be 90%
  fig.width = 8,
  fig.height = 6, 
  out.width = "90%"
)

options(
  ggplot2.countinuous.colour = "viridis",
  ggplot2.countinuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_bw() + theme(legend.position = "bottom"))

```


## problem 1 
```{r}
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))

## writing function filling
  filling = function(vector) {
  if (!is.vector(vector)) {
    stop("Input should be a vector.")
  }
  if (is.numeric(vector)) {
    vector = vector %>% 
      replace_na(mean(vector[!is.na(vector)]))
  }  else if(is.character(vector)) {
    vector = vector %>% replace_na ("virginica")
  } 
  }
  iris_without_missing = map_df(.x = iris_with_missing, ~ filling(.x))
  iris_without_missing

```

## problem 2 
```{r message = FALSE}
## dataframe of all the datasets
total_dataset = list.files(path = "./data/", full.names = TRUE) %>% 
  as.data.frame() %>% 
  rename("file_id" = ".") %>% 
  mutate(file_id = as.character(file_id))

##Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time  
 tidy_data =  
 total_dataset %>% 
  mutate("data" = purrr::map(.x = pull(., file_id), read_csv),
         file_id = str_extract(file_id, "[conexp]{3}_[0-9]{2}")) %>%
  unnest() %>% 
    pivot_longer(
    names_prefix = "week_",
    cols = week_1:week_8,
    names_to = "week",
    values_to = "observations"
  ) %>%   
  separate(file_id, into = c("arm", "subject_id"), sep = "_") %>% 
  mutate(arm = recode(arm , "con" = "control", "exp" = "experimental"),
         subject_id = as.factor(subject_id),
         arm = as.factor(arm)) %>% 
  select(subject_id, arm, everything())

## Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

tidy_data %>% 
  ggplot(aes(x = week, y = observations, group = subject_id)) +
  geom_line(aes(color = subject_id)) +
  facet_grid( . ~ arm) +
  labs(
    title = " Spaghetti Plot showing Observations on Each Subject Over Time,  ",
    x = "Week",
    y = " Weekly Observations"
  )

```
Interpretation: 
Based on the plot above, we can see that on average that as time went by, the weekly observation is relative stable in control group. On the contrary, there was a increasing trend in weekly observation in experimental group. Overall, experimental group showed larger observation than control group.


## problem 3 

Generate 10000 datasets from the model
```{r message=FALSE}
# function for doing regression simulation and set parameters mentioned
sim_regression = function(n = 30, beta0 = 2, beta1 = 0) {
    sim_data = tibble(
    x = rnorm(n, mean = 1, sd = 1),
    y = beta0 + beta1 * x + rnorm(n, mean = 0, sd = sqrt(50))
  )
  
  ls_fit = lm(y ~ x, data = sim_data)
  
  ls_with_pvalue = broom::tidy(ls_fit)
  
    tibble(
    beta1_hat = ls_with_pvalue[[2, "estimate"]],
    p_value = ls_with_pvalue[[2, "p.value"]]
  )
}

## 10000 runs of dataset
  simulation = 
  tibble(beta1 = 0:6) %>% 
  mutate(beta1_hat = map(.x = beta1, ~rerun(100, sim_regression(beta1 = .x)))) %>% 
  unnest()  %>%
  unnest

```


```{r}
## plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of Î²2 on the x axis.
  simulation %>% 
  mutate(decision = case_when(
    p_value <= 0.05 ~ "reject",
    p_value > 0.05 ~ "failed to reject",
    TRUE ~ ""
  )) %>% 
  group_by(beta1, decision) %>% 
  summarize(reject_proportion = n()/10000) %>% 
  filter(decision == "reject") %>%
  ggplot(aes(x = beta1, y = reject_proportion)) +
  geom_point() + geom_line() +
  scale_x_continuous(breaks = c(0, 1, 2, 3, 4, 5, 6)) +
  labs(
    title = "Line Plot Showing the Proportion of Times the Null being Rejected",
    x = "Beta 1",
    y = "Proportion of Times the Null being Rejected"
  )
  
```
interpretation: Based on the plot, we can see that there is a positive correlation between power and beta1. As the effect size increases, power also increases

```{r}
simulation %>%  
 mutate(reject = case_when (
    p_value <= 0.05 ~ "reject",
    p_value > 0.05 ~ "failed to reject",
    TRUE ~ ""
  )) %>% 
  group_by(beta1) %>% 
  mutate(average_overall = mean(beta1_hat)) %>% 
  ungroup() %>% 
  group_by(beta1, reject) %>% 
  mutate(average_reject = mean(beta1_hat)) %>% 
  filter(reject == "reject") %>% 
  ungroup() %>% 
  select(beta1, average_reject, average_overall) %>% 
  distinct() %>% 
  pivot_longer(
    cols = average_reject:average_overall,
    names_to = "type",
    values_to = "value",
    names_prefix = "average_"
  ) %>% 
  ggplot(aes(x = beta1, y = value, color = type)) +
  geom_point() + geom_line(aes(group = type)) +
  scale_x_continuous(breaks = c(0, 1, 2, 3, 4, 5, 6)) +
  scale_color_discrete("Type of Average Estimated Beta 1") +
  labs(
    title = "Line Plot of the True Beta 1 and Average Estimated Beta 1",
    x = "True Beta 1",
    y = "Estimated Average Beta 1"
  )
```

Interpretation: 
According to the plot, we can see that the points at each true beta value for overall and rejected are somewhat different from each other. Although some of the point deviated far from each other, rejecting H0 means that the probability of estimation is not what is true is larger. Meanwhile som eof the points are closer together, and this could be caused by the large sample size we simulated, which will cause the randomly generated data to behavior more similar to the true beta.
